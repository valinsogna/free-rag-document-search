# üÜì Setup Completo - Versione GRATUITA con Ollama

## üéØ Questa versione √® 100% GRATIS!

Nessun costo API, tutto gira localmente sul tuo PC.

---

## üìã Prerequisiti

- Python 3.8+
- 8GB+ RAM (16GB consigliato per modelli grandi)
- 10GB spazio disco per modelli

---

## üöÄ Installazione Passo-Passo

### 1Ô∏è‚É£ Installa Ollama

**Windows:**
```bash
# Scarica da: https://ollama.ai/download/windows
# Esegui l'installer
```

**Mac:**
```bash
# Scarica da: https://ollama.ai/download/mac
# Oppure con Homebrew:
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2Ô∏è‚É£ Verifica installazione

```bash
ollama --version
```

### 3Ô∏è‚É£ Scarica i modelli (GRATIS!)

```bash
# Modello per embeddings (necessario)
ollama pull nomic-embed-text

# Scegli UN modello LLM:

# Opzione 1: Leggero e veloce (CONSIGLIATO per iniziare)
ollama pull llama3.2

# Opzione 2: Bilanciato qualit√†/velocit√†
ollama pull mistral

# Opzione 3: Pi√π potente (richiede pi√π RAM)
ollama pull llama3.1:8b

# Opzione 4: Efficiente di Microsoft
ollama pull phi3
```

**Verifica modelli scaricati:**
```bash
ollama list
```

Dovresti vedere:
```
NAME                    ID              SIZE
nomic-embed-text        latest          274 MB
llama3.2                latest          2.0 GB
```

### 4Ô∏è‚É£ Installa dipendenze Python

```bash
# Crea virtual environment
python -m venv venv

# Attiva (Linux/Mac)
source venv/bin/activate

# Attiva (Windows)
venv\Scripts\activate

# Installa dipendenze
pip install -r requirements_free.txt
```

### 5Ô∏è‚É£ Prepara documenti

```bash
mkdir documents
# Copia i tuoi PDF, DOCX, TXT qui
```

### 6Ô∏è‚É£ Esegui il sistema

```bash
python rag_free_ollama.py
```

---

## üéÆ Come Usare

```
ü§î Domanda: Quali sono i progetti principali nel mio CV?

üí≠ Thinking...
üí° Risposta: I progetti principali sono...

üìö Fonti:
   1. curriculum.pdf
```

**Comandi speciali:**
- `exit` - Esci
- `model` - Cambia modello LLM
- `q` - Esci

---

## üîß Configurazione Avanzata

### Cambiare modello nel codice

```python
rag = FreeLocalRAG(
    documents_path="./documents",
    model_name="mistral"  # Cambia qui!
)
```

### Modelli disponibili

| Modello | Dimensione | RAM | Velocit√† | Qualit√† | Uso |
|---------|------------|-----|----------|---------|-----|
| `llama3.2` | 2GB | 8GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Generale |
| `mistral` | 4GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Bilanciato |
| `llama3.1:8b` | 5GB | 16GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Qualit√† |
| `phi3` | 2.3GB | 8GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Efficiente |
| `qwen2.5` | varia | 8GB+ | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Multilingua |

### Ottimizzare le prestazioni

```python
# Nel file rag_free_ollama.py modifica:

# 1. Riduci chunk size per risposte pi√π veloci
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # era 1000
    chunk_overlap=100
)

# 2. Riduci numero di chunks recuperati
retriever=self.vectorstore.as_retriever(
    search_kwargs={"k": 2}  # era 3
)

# 3. Usa modello pi√π leggero
model_name="llama3.2"  # invece di llama3.1:8b
```
---

## Usa la modalit√† Web UI con Streamlit
   ```bash
   pip install streamlit
   streamlit run app_streamlit_free.py
   ```

---

## üêõ Troubleshooting

### Problema: "Ollama non trovato"
```bash
# Verifica che Ollama sia installato
ollama --version

# Avvia il servizio
ollama serve
```

### Problema: "Modello non trovato"
```bash
# Scarica il modello
ollama pull llama3.2
ollama pull nomic-embed-text
```

### Problema: "Out of memory"
- Chiudi altre applicazioni
- Usa un modello pi√π piccolo (`llama3.2` invece di `llama3.1:8b`)
- Riduci `chunk_size` nel codice

### Problema: "Troppo lento"
- Usa un modello pi√π piccolo
- Riduci il numero di documenti
- Riduci `k` nel retriever (meno chunks)

---


## üîÑ Prossimi Passi

2. **Deploy locale:**
   - Crea Docker container
   - Condividi sulla rete locale

3. **Espandi funzionalit√†:**
   - Aggiungi pi√π formati file
   - Implementa conversazione multi-turn
   - Aggiungi analytics

---

## üìû Supporto

Se hai problemi:
1. Verifica che Ollama sia in esecuzione: `ollama list`
2. Verifica che i modelli siano scaricati
3. Controlla i log per errori

---

## ‚ö° Quick Reference

```bash
# Setup completo in 4 comandi
ollama pull nomic-embed-text
ollama pull llama3.2
pip install -r requirements.txt
python rag_free_ollama.py
```

üéâ **Tutto GRATIS!** Nessun costo nascosto, nessuna API key richiesta!